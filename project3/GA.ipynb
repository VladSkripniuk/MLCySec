{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('essaysense/datasets/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_adversarial_examples import glove_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = 'essaysense/datasets/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 2196007  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove_model = glove_utils.loadGloveModel(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class HyperParameters:\n",
    "    \"\"\"Hyper-parameters of this project.\n",
    "\n",
    "    This is a class holding necessary hyperparameters of this project. Instan-\n",
    "    tiation of the class can get all of the parameters. Note that property\n",
    "    protection is not constructed, so DO NOT change the values unless you know\n",
    "    what you are doing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_epochs = 700  # General training epochs.\n",
    "#         self.w_dim = 50  # Word embedding dimension.\n",
    "        self.w_dim = 300  # Word embedding dimension.\n",
    "        self.s_len = 20  # Sentence length in the sentence-level models.\n",
    "        self.e_len = 60  # Essay length in the sentence-level models.\n",
    "        self.w_window_len = 5  # Convolution window size of word level.\n",
    "        self.s_window_len = 3  # Convolution window size of sentence level.\n",
    "        self.w_convunits_size = 64  # Convolution unit number of word level.\n",
    "        self.s_convunits_size = 32 # Convolution unit number of sentence level.\n",
    "        self.hidden_size = 100  # Dense layer size of sentence-level models.\n",
    "        self.batch_size = 20  # Batch size.\n",
    "        self.learning_rate = 0.006  # Initial learning rate.\n",
    "        self.dropout_keep_prob = 0.3  # Dropout rate.\n",
    "        self.d_e_len = 500  # Essay length in the document-level models.\n",
    "        self.lstm_hidden_size = 150  # Dense layer size of LSTM models.\n",
    "        self.cnn_lstm_convunits_size = 80  # Conv units of CNN-LSTM models.\n",
    "        self.cnn_lstm_att_pool_size = 50  # Attention pool size.\n",
    "\n",
    "class ProjectPaths:\n",
    "    \"\"\"Project paths of the application.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.aes_root = \"essaysense\"  # Temporarily\n",
    "        self.tfmetadata = os.path.join(self.aes_root, \"tfmetadata\")\n",
    "        self.datasets_root = os.path.join(self.aes_root, \"datasets\")\n",
    "        self.asap = os.path.join(self.datasets_root, \"training_set_rel3.tsv\")\n",
    "        self.asap_train = os.path.join(self.datasets_root, \"train.tsv\")\n",
    "        self.asap_dev = os.path.join(self.datasets_root, \"dev.tsv\")\n",
    "        self.asap_test = os.path.join(self.datasets_root, \"test.tsv\")\n",
    "        self.asap_url = \"http://p2u3jfd2o.bkt.clouddn.com/datasets/training_set_rel3.tsv\"\n",
    "#         self.glove = os.path.join(self.datasets_root, \"glove.6B.50d.txt\")\n",
    "        self.glove = os.path.join(self.datasets_root, \"glove.840B.300d.txt\")\n",
    "        self.glove_url = \"http://p2u3jfd2o.bkt.clouddn.com/datasets/glove.6B.50d.txt\"\n",
    "\n",
    "    def model(self, model_name):\n",
    "        return os.path.join(self.tfmetadata, model_name, \"model.ckpt\")\n",
    "\n",
    "    def model_ckpt(self, model_name):\n",
    "        return os.path.join(self.tfmetadata, model_name)\n",
    "\n",
    "    def summary(self, model_name):\n",
    "        return os.path.join(self.tfmetadata, model_name, \"summary\")\n",
    "\n",
    "\n",
    "# Variables to export.\n",
    "hp = hyperparameters = HyperParameters()\n",
    "paths = ProjectPaths()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn as tfrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_graph():\n",
    "    tf.reset_default_graph()\n",
    "    essays = tf.placeholder(tf.float32, [None, hp.d_e_len,\n",
    "                                         hp.w_dim])\n",
    "    scores = tf.placeholder(tf.float32, [None])\n",
    "    keep_prob = tf.placeholder_with_default(tf.constant(1.0, dtype=tf.float32), ())\n",
    "\n",
    "    # Long Short-Term Memory layer\n",
    "    lstm_cell = tfrnn.BasicLSTMCell(num_units=hp.lstm_hidden_size)\n",
    "    lstm_cell = tfrnn.DropoutWrapper(\n",
    "        cell=lstm_cell,\n",
    "        output_keep_prob=keep_prob)\n",
    "    init_state = lstm_cell.zero_state(hp.batch_size, dtype=tf.float32)\n",
    "    lstm, _ = tf.nn.dynamic_rnn(lstm_cell, essays, dtype=tf.float32)\n",
    "\n",
    "    # Mean over Time pooling\n",
    "    mot = tf.reduce_mean(lstm, axis=1)\n",
    "\n",
    "    # Dense layer\n",
    "    dense = tf.layers.dense(inputs=mot, units=1, activation=tf.nn.sigmoid)\n",
    "\n",
    "    # Prediction and Loss\n",
    "    preds = tf.reshape(dense, [-1])\n",
    "    loss = tf.losses.mean_squared_error(scores, preds)\n",
    "\n",
    "    return (essays,\n",
    "            scores,\n",
    "            keep_prob,\n",
    "            loss,\n",
    "            preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_range = {1: (2, 12),\n",
    "               2: (1, 6),\n",
    "               3: (0, 3),\n",
    "               4: (0, 3),\n",
    "               5: (0, 4),\n",
    "               6: (0, 4),\n",
    "               7: (0, 30),\n",
    "               8: (0, 60)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_range_min = np.array([2, 1, 0, 0, 0, 0, 0, 0])\n",
    "score_range_max = np.array([12, 6, 3, 3, 4, 4, 30, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(scores, essay_set):\n",
    "    mini = np.choose(essay_set-1,score_range_min)\n",
    "    maxi = np.choose(essay_set-1,score_range_max)\n",
    "    \n",
    "    return (scores - mini) / (maxi - mini)\n",
    "\n",
    "def expand(scores, essay_set):\n",
    "    mini = np.choose(essay_set-1,score_range_min)\n",
    "    maxi = np.choose(essay_set-1,score_range_max)\n",
    "    \n",
    "    return np.array(np.round(scores * (maxi - mini) + mini),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_score(score, essay_set):\n",
    "    return (float(score) - score_range[essay_set][0]) / float(score_range[essay_set][1] - score_range[essay_set][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_level_tokenize(essay_text):\n",
    "    essay_text = essay_text.encode('ascii', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    essay_text = essay_text.lower()  # Use lower-cases for word embeddings.\n",
    "    essay_text = essay_text.replace('/', ' / ')\n",
    "    essay_text = essay_text.replace('@', ' ')\n",
    "    essay_text = essay_text.replace('.', ' . ')\n",
    "    essay_text = essay_text.replace('-', ' - ')\n",
    "    essay = nltk.word_tokenize(essay_text)\n",
    "    essay = list(map(lambda x : (x[:-1] if x[-1].isdigit() else x), essay))\n",
    "    essay = list(filter(lambda x: len(x) > 0, essay))\n",
    "    return essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, labels, corrections=False):\n",
    "    cnt_corrections = 0\n",
    "    set_size = len(data)\n",
    "    i_item = 0\n",
    "    while True:\n",
    "        if i_item >= set_size:\n",
    "            i_item = 0\n",
    "        \n",
    "        if type(data) is pd.DataFrame:\n",
    "            item = data.iloc[i_item]\n",
    "            label = labels.iloc[i_item]\n",
    "            essay_text = document_level_tokenize(item['essay'])\n",
    "        else:\n",
    "            item = data[i_item]\n",
    "            label = labels[i_item]\n",
    "            essay_text = document_level_tokenize(item)\n",
    "            \n",
    "        embedded = np.zeros([hp.d_e_len, hp.w_dim])\n",
    "        for i in range(min(len(essay_text), hp.d_e_len)):\n",
    "            if corrections and essay_text[i] in corrections_dict.keys():\n",
    "                essay_text[i] = corrections_dict[essay_text[i]]\n",
    "            \n",
    "            embedded[i] = glove_vectors.get(essay_text[i], np.zeros(hp.w_dim))\n",
    "            \n",
    "        i_item += 1\n",
    "        \n",
    "        if type(data) is pd.DataFrame: \n",
    "            yield (embedded, normalize_score(label, item[\"essay_set\"]))\n",
    "        else:\n",
    "            yield (embedded, normalize_score(label, 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(gen, size_demand):\n",
    "    essays_batched = []\n",
    "    scores_batched = []\n",
    "    for _ in range(size_demand):\n",
    "        next_item = next(gen)  # Generate next item\n",
    "        essays_batched.append(next_item[0])\n",
    "        scores_batched.append(next_item[1])\n",
    "    essays_batched = np.array(essays_batched)\n",
    "    scores_batched = np.array(scores_batched)\n",
    "    return essays_batched, scores_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AES():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def predict(self, sess, list_essays):\n",
    "        all_embedded = np.zeros([len(list_essays), hp.d_e_len, hp.w_dim])\n",
    "        all_preds = np.zeros([len(list_essays), 2])\n",
    "        \n",
    "        for index in range(len(list_essays)):\n",
    "            essay = list_essays[index]\n",
    "            embedded = np.zeros([hp.d_e_len, hp.w_dim])\n",
    "            for i in range(min(len(essay), hp.d_e_len)):\n",
    "                embedded[i] = glove_model.get(full_inv_dict[essay[i]], np.zeros([hp.w_dim]))\n",
    "            all_embedded[index] = embedded\n",
    "\n",
    "        preds_got = sess.run(preds, feed_dict={essays: all_embedded, keep_prob: 1.0})\n",
    "        all_preds[:,0] = 1 - preds_got\n",
    "        all_preds[:,1] = preds_got\n",
    "        \n",
    "        return all_preds\n",
    "    \n",
    "aes = AES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-dc35827dab16>:9: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "(essays,\n",
    " scores,\n",
    " keep_prob,\n",
    " loss,\n",
    " preds) = define_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/LSTM_2_wo_corr\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.restore(sess, './models/LSTM_2_wo_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dict = np.load('aux_files/full_dict.npy').reshape(1,)[0]\n",
    "full_inv_dict = np.load('aux_files/full_inv_dict.npy').reshape(1,)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything = \" \".join(X['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = document_level_tokenize(everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dict = dict()\n",
    "full_inv_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = max_vocab_size = len(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38624"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset built !\n"
     ]
    }
   ],
   "source": [
    "full_dict['UNK'] = max_vocab_size\n",
    "full_inv_dict = dict()\n",
    "full_inv_dict[max_vocab_size] = 'UNK'\n",
    "for idx, word in enumerate(all_unique_words):\n",
    "    if idx < max_vocab_size:\n",
    "        full_inv_dict[idx] = word\n",
    "        full_dict[word] = idx\n",
    "    else:\n",
    "        print('whoops')\n",
    "print('Dataset built !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aux_files/full_dict.npy', full_dict)\n",
    "np.save('aux_files/full_inv_dict.npy', full_inv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not found words =  10235\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings, _ = glove_utils.create_embeddings_matrix(glove_model, full_dict, full_dict)\n",
    "# save the glove_embeddings matrix\n",
    "np.save('aux_files/embeddings_glove_%d.npy' %(MAX_VOCAB_SIZE), glove_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aux_files/embeddings_glove_%d.npy' %(MAX_VOCAB_SIZE), glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 65713  words loaded!\n",
      "Number of not found words =  20011\n"
     ]
    }
   ],
   "source": [
    "# Load the counterfitted-vectors (used by our attack)\n",
    "glove2 = glove_utils.loadGloveModel('essaysense/datasets/counter-fitted-vectors.txt')\n",
    "# create embeddings matrix for our vocabulary\n",
    "counter_embeddings, missed = glove_utils.create_embeddings_matrix(glove2, full_dict, full_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done\n"
     ]
    }
   ],
   "source": [
    "# save the embeddings for both words we have found, and words that we missed.\n",
    "np.save(('aux_files/embeddings_counter_%d.npy' %(MAX_VOCAB_SIZE)), counter_embeddings)\n",
    "np.save(('aux_files/missed_embeddings_counter_%d.npy' %(MAX_VOCAB_SIZE)), missed)\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words to `good` are :\n",
      "['alright', 'well', 'nice', 'decent', 'bueno', 'best', 'allright', 'presentable', 'goods', 'fine']\n"
     ]
    }
   ],
   "source": [
    "# MAX_VOCAB_SIZE = 50000\n",
    "embedding_matrix = np.load(('aux_files/embeddings_counter_%d.npy' %(MAX_VOCAB_SIZE)))\n",
    "missed = np.load(('aux_files/missed_embeddings_counter_%d.npy' %(MAX_VOCAB_SIZE)))\n",
    "c_ = -2*np.dot(embedding_matrix.T , embedding_matrix)\n",
    "a = np.sum(np.square(embedding_matrix), axis=0).reshape((1,-1))\n",
    "b = a.T\n",
    "dist = a+b+c_\n",
    "np.save(('aux_files/dist_counter_%d.npy' %(MAX_VOCAB_SIZE)), dist)\n",
    "\n",
    "# Try an example\n",
    "src_word = full_dict['good']\n",
    "neighbours, neighbours_dist = glove_utils.pick_most_similar_words(src_word, dist)\n",
    "print('Closest words to `good` are :')\n",
    "result_words = [full_inv_dict[x] for x in neighbours]\n",
    "print(result_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nlp_adversarial_examples import data_utils, glove_utils, models, display_utils\n",
    "from nlp_adversarial_examples.goog_lm import LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_adversarial_examples import lm_data_utils, lm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=MAX_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = np.load('aux_files/dist_counter_%d.npy' %VOCAB_SIZE)\n",
    "# Prevent returning 0 as most similar word because it is not part of the dictionary\n",
    "dist_mat[0,:] = 100000\n",
    "dist_mat[:,0] = 100000\n",
    "\n",
    "skip_list = np.load('aux_files/missed_embeddings_counter_%d.npy' %VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM vocab loading done\n",
      "WARNING:tensorflow:From /raid/mlcysec/student_directories/securitymonks/essaysense/nlp_adversarial_examples/lm_utils.py:21: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recovering Graph goog_lm/graph-2016-09-10.pbtxt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering checkpoint goog_lm/ckpt-*\n"
     ]
    }
   ],
   "source": [
    "goog_lm = LM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to `play` are ['playing', 'gaming', 'games', 'toy', 'game', 'plaything', 'cheek', 'gambling', 'toys', 'replay', 'stake', 'plays', 'gamble', 'casino', 'sets', 'set', 'reproduce', 'exostied', 'idsfun', 'lalaby']\n"
     ]
    }
   ],
   "source": [
    "src_word = full_dict['play']\n",
    "nearest, nearest_dist = glove_utils.pick_most_similar_words(src_word, dist_mat,20)\n",
    "nearest_w = [full_inv_dict[x] for x in nearest]\n",
    "print('Closest to `%s` are %s' %(full_inv_dict[src_word], nearest_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most probable is  plays\n"
     ]
    }
   ],
   "source": [
    "prefix = 'play'\n",
    "suffix = 'with'\n",
    "lm_preds = goog_lm.get_words_probs(prefix, nearest_w, suffix)\n",
    "print('most probable is ', nearest_w[np.argmax(lm_preds)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_adversarial_examples.attacks import GreedyAttack, GeneticAtack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.dict = full_dict\n",
    "        self.inv_dict = full_inv_dict\n",
    "    \n",
    "dataset = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 32\n",
    "n1 = 8\n",
    "\n",
    "ga_attack = GeneticAtack(sess, aes, aes, aes, dataset, dist_mat, \n",
    "                                  skip_list,\n",
    "                                  goog_lm, max_iters=30, \n",
    "                                    pop_size=pop_size, n1 = n1,\n",
    "                                  n2 = 4,\n",
    "                                 use_lm = True, use_suffix=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[X['essay_set']==4].iloc[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author ends the story with this conclusion to show that @PERSON1 is very passionate about connecting her life in the United States, to how life was in Vietnam and bringing them together somehow. In the story, @PERSON1 talks about how she misses home and how she is still trying to adapt to her new life away from the home she knew, when saeng says that she has failed the test and wants to take it again, she means that she wants to show that she can survive in the United States without disappointing her mother. When saeng spent @MONEY1 on a plant, her mom was in disbelief because Saeng knows how hard it is to survive. When the conclusion says,\\x94I will take the test again,\\x94 it shows that Saeng is willing try to make it in the United States from Vietnam, proving that she can make a better life for herself and her family by working hard. While still keeping trditions in Vietnam alive. Saeng simply wanted to make her new home feel like Vietnam by buying a plant she knew her mother would like.'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X['essay_set']==4]['essay'].iloc[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Have you seen a magazine, book, movies, etc., that are found affensive? What experiences did you have? Here is my opinion on if I think that those books should be removed or not.       I have noticed that some movies are affensive to other people. Like for an example, the @CAPS1 movies, books is about @CAPS2 and some people don't believe in them or they don't like the movie so I do kind of see no point of making a movie that is about someone that is not real. However, some movies are okay for some people and their age. The movies that are rated '@CAPS3' are for the people who shouldn't be watching it yet like kids under the age.      Magazines though do have some type of thing that I think that is affensive to other people. Like, I don't remember the name of them but they would have sections that would talk bad about another person like one of the kids would talk about the president or something like that. So I think some magazines should be removed off the shelves.      The books however, I don't see a reason why to have them removed off the shelves. I don't think the books seem to be affensive as much as graphical. There could be some books out there that might be affensive to people though, like the ones that talk about a family that has no money or talk bad about them. For an example, you would be able to find a book and read it and find out that it is talking bout things that you don't want to know yet or want to know.      So as you can see I have made an opinion on what they should do either keep the books, magazines, or movies off the shelf. But then I do see it the other way to. Some of us do like those kind of movies or book or even magazines and think that is very cool or interesting\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X['essay_set']==2]['essay'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join([self.dataset.inv_dict[token] for token in elite[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'you',\n",
       " 'seen',\n",
       " 'a',\n",
       " 'magazine',\n",
       " ',',\n",
       " 'book',\n",
       " ',',\n",
       " 'movies',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " ',',\n",
       " 'that',\n",
       " 'are',\n",
       " 'found',\n",
       " 'affensive',\n",
       " '?',\n",
       " 'what',\n",
       " 'experiences',\n",
       " 'did',\n",
       " 'you',\n",
       " 'have',\n",
       " '?',\n",
       " 'here',\n",
       " 'is',\n",
       " 'my',\n",
       " 'opinion',\n",
       " 'on',\n",
       " 'if',\n",
       " 'i',\n",
       " 'think',\n",
       " 'that',\n",
       " 'those',\n",
       " 'books',\n",
       " 'should',\n",
       " 'be',\n",
       " 'removed',\n",
       " 'or',\n",
       " 'not',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'noticed',\n",
       " 'that',\n",
       " 'some',\n",
       " 'movies',\n",
       " 'are',\n",
       " 'affensive',\n",
       " 'to',\n",
       " 'other',\n",
       " 'people',\n",
       " '.',\n",
       " 'like',\n",
       " 'for',\n",
       " 'an',\n",
       " 'example',\n",
       " ',',\n",
       " 'the',\n",
       " 'caps',\n",
       " 'movies',\n",
       " ',',\n",
       " 'books',\n",
       " 'is',\n",
       " 'about',\n",
       " 'caps',\n",
       " 'and',\n",
       " 'some',\n",
       " 'people',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'believe',\n",
       " 'in',\n",
       " 'them',\n",
       " 'or',\n",
       " 'they',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'like',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'so',\n",
       " 'i',\n",
       " 'do',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'see',\n",
       " 'no',\n",
       " 'point',\n",
       " 'of',\n",
       " 'making',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'is',\n",
       " 'about',\n",
       " 'someone',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'real',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'some',\n",
       " 'movies',\n",
       " 'are',\n",
       " 'okay',\n",
       " 'for',\n",
       " 'some',\n",
       " 'people',\n",
       " 'and',\n",
       " 'their',\n",
       " 'age',\n",
       " '.',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'that',\n",
       " 'are',\n",
       " 'rated',\n",
       " \"'\",\n",
       " 'caps',\n",
       " \"'\",\n",
       " 'are',\n",
       " 'for',\n",
       " 'the',\n",
       " 'people',\n",
       " 'who',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'watching',\n",
       " 'it',\n",
       " 'yet',\n",
       " 'like',\n",
       " 'kids',\n",
       " 'under',\n",
       " 'the',\n",
       " 'age',\n",
       " '.',\n",
       " 'magazines',\n",
       " 'though',\n",
       " 'do',\n",
       " 'have',\n",
       " 'some',\n",
       " 'type',\n",
       " 'of',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'i',\n",
       " 'think',\n",
       " 'that',\n",
       " 'is',\n",
       " 'affensive',\n",
       " 'to',\n",
       " 'other',\n",
       " 'people',\n",
       " '.',\n",
       " 'like',\n",
       " ',',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'remember',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'them',\n",
       " 'but',\n",
       " 'they',\n",
       " 'would',\n",
       " 'have',\n",
       " 'sections',\n",
       " 'that',\n",
       " 'would',\n",
       " 'talk',\n",
       " 'bad',\n",
       " 'about',\n",
       " 'another',\n",
       " 'person',\n",
       " 'like',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'would',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'the',\n",
       " 'president',\n",
       " 'or',\n",
       " 'something',\n",
       " 'like',\n",
       " 'that',\n",
       " '.',\n",
       " 'so',\n",
       " 'i',\n",
       " 'think',\n",
       " 'some',\n",
       " 'magazines',\n",
       " 'should',\n",
       " 'be',\n",
       " 'removed',\n",
       " 'off',\n",
       " 'the',\n",
       " 'shelves',\n",
       " '.',\n",
       " 'the',\n",
       " 'books',\n",
       " 'however',\n",
       " ',',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'see',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'to',\n",
       " 'have',\n",
       " 'them',\n",
       " 'removed',\n",
       " 'off',\n",
       " 'the',\n",
       " 'shelves',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'think',\n",
       " 'the',\n",
       " 'books',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'be',\n",
       " 'affensive',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'graphical',\n",
       " '.',\n",
       " 'there',\n",
       " 'could',\n",
       " 'be',\n",
       " 'some',\n",
       " 'books',\n",
       " 'out',\n",
       " 'there',\n",
       " 'that',\n",
       " 'might',\n",
       " 'be',\n",
       " 'affensive',\n",
       " 'to',\n",
       " 'people',\n",
       " 'though',\n",
       " ',',\n",
       " 'like',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'that',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'a',\n",
       " 'family',\n",
       " 'that',\n",
       " 'has',\n",
       " 'no',\n",
       " 'money',\n",
       " 'or',\n",
       " 'talk',\n",
       " 'bad',\n",
       " 'about',\n",
       " 'them',\n",
       " '.',\n",
       " 'for',\n",
       " 'an',\n",
       " 'example',\n",
       " ',',\n",
       " 'you',\n",
       " 'would',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'find',\n",
       " 'a',\n",
       " 'book',\n",
       " 'and',\n",
       " 'read',\n",
       " 'it',\n",
       " 'and',\n",
       " 'find',\n",
       " 'out',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'talking',\n",
       " 'bout',\n",
       " 'things',\n",
       " 'that',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'know',\n",
       " 'yet',\n",
       " 'or',\n",
       " 'want',\n",
       " 'to',\n",
       " 'know',\n",
       " '.',\n",
       " 'so',\n",
       " 'as',\n",
       " 'you',\n",
       " 'can',\n",
       " 'see',\n",
       " 'i',\n",
       " 'have',\n",
       " 'made',\n",
       " 'an',\n",
       " 'opinion',\n",
       " 'on',\n",
       " 'what',\n",
       " 'they',\n",
       " 'should',\n",
       " 'do',\n",
       " 'either',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'books',\n",
       " ',',\n",
       " 'magazines',\n",
       " ',',\n",
       " 'or',\n",
       " 'movies',\n",
       " 'off',\n",
       " 'the',\n",
       " 'shelf',\n",
       " '.',\n",
       " 'but',\n",
       " 'then',\n",
       " 'i',\n",
       " 'do',\n",
       " 'see',\n",
       " 'it',\n",
       " 'the',\n",
       " 'other',\n",
       " 'way',\n",
       " 'to',\n",
       " '.',\n",
       " 'some',\n",
       " 'of',\n",
       " 'us',\n",
       " 'do',\n",
       " 'like',\n",
       " 'those',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'movies',\n",
       " 'or',\n",
       " 'book',\n",
       " 'or',\n",
       " 'even',\n",
       " 'magazines',\n",
       " 'and',\n",
       " 'think',\n",
       " 'that',\n",
       " 'is',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'or',\n",
       " 'interesting']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_level_tokenize(X[X['essay_set']==4]['essay'].iloc[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(['are so many small things' in x for x in X['essay']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_id                                                       4777\n",
       "essay_set                                                         2\n",
       "essay             Different Then Everyone Else     @CAPS1 do peo...\n",
       "rater1_domain1                                                    3\n",
       "rater2_domain1                                                    3\n",
       "rater3_domain1                                                  NaN\n",
       "domain1_score                                                     3\n",
       "rater1_domain2                                                    2\n",
       "rater2_domain2                                                    3\n",
       "domain2_score                                                     2\n",
       "rater1_trait1                                                   NaN\n",
       "rater1_trait2                                                   NaN\n",
       "rater1_trait3                                                   NaN\n",
       "rater1_trait4                                                   NaN\n",
       "rater1_trait5                                                   NaN\n",
       "rater1_trait6                                                   NaN\n",
       "rater2_trait1                                                   NaN\n",
       "rater2_trait2                                                   NaN\n",
       "rater2_trait3                                                   NaN\n",
       "rater2_trait4                                                   NaN\n",
       "rater2_trait5                                                   NaN\n",
       "rater2_trait6                                                   NaN\n",
       "rater3_trait1                                                   NaN\n",
       "rater3_trait2                                                   NaN\n",
       "rater3_trait3                                                   NaN\n",
       "rater3_trait4                                                   NaN\n",
       "rater3_trait5                                                   NaN\n",
       "rater3_trait6                                                   NaN\n",
       "Name: 3582, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[3582]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['different',\n",
       " 'then',\n",
       " 'everyone',\n",
       " 'else',\n",
       " 'caps',\n",
       " 'do',\n",
       " 'people',\n",
       " 'find',\n",
       " 'small',\n",
       " 'things',\n",
       " 'offensive',\n",
       " '?',\n",
       " 'my',\n",
       " 'opinion',\n",
       " 'is',\n",
       " 'the',\n",
       " 'everyone',\n",
       " 'as',\n",
       " 'their',\n",
       " 'own',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'what',\n",
       " 'can',\n",
       " 'be',\n",
       " 'offensive',\n",
       " 'to',\n",
       " 'them',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'so',\n",
       " 'many',\n",
       " 'small',\n",
       " 'things',\n",
       " 'the',\n",
       " 'can',\n",
       " 'be',\n",
       " 'offensive',\n",
       " 'like',\n",
       " 'books',\n",
       " ',',\n",
       " 'movies',\n",
       " ',',\n",
       " 'and',\n",
       " 'music',\n",
       " '.',\n",
       " 'its',\n",
       " 'not',\n",
       " 'horrible',\n",
       " 'or',\n",
       " 'bad',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'book',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelf',\n",
       " 'the',\n",
       " 'you',\n",
       " 'might',\n",
       " 'think',\n",
       " 'its',\n",
       " 'offensive',\n",
       " '.',\n",
       " 'i',\n",
       " 'find',\n",
       " 'it',\n",
       " 'a',\n",
       " 'little',\n",
       " 'rude',\n",
       " 'to',\n",
       " 'discriminate',\n",
       " 'someone',\n",
       " 'for',\n",
       " 'thier',\n",
       " 'one',\n",
       " 'belonging',\n",
       " '.',\n",
       " 'books',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'way',\n",
       " 'to',\n",
       " 'learned',\n",
       " 'new',\n",
       " 'things',\n",
       " 'about',\n",
       " 'someone',\n",
       " '.',\n",
       " 'i',\n",
       " 'think',\n",
       " 'sometimes',\n",
       " 'books',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'more',\n",
       " 'then',\n",
       " 'what',\n",
       " 'can',\n",
       " 'a',\n",
       " 'person',\n",
       " 'with',\n",
       " 'there',\n",
       " 'own',\n",
       " 'words',\n",
       " 'can',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'so',\n",
       " 'many',\n",
       " 'books',\n",
       " 'the',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'little',\n",
       " 'offensive',\n",
       " 'to',\n",
       " 'people',\n",
       " 'and',\n",
       " 'even',\n",
       " 'kids',\n",
       " '.',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'a',\n",
       " 'parent',\n",
       " 'would',\n",
       " \"n't\",\n",
       " 'like',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'book',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelf',\n",
       " 'about',\n",
       " \"'\",\n",
       " 'caps',\n",
       " \"'\",\n",
       " ',',\n",
       " 'when',\n",
       " 'they',\n",
       " 'are',\n",
       " 'getting',\n",
       " 'caps',\n",
       " '.',\n",
       " 'sure',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'thing',\n",
       " 'to',\n",
       " 'read',\n",
       " 'about',\n",
       " 'but',\n",
       " ',',\n",
       " 'really',\n",
       " 'parents',\n",
       " 'think',\n",
       " 'about',\n",
       " 'it',\n",
       " '.',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'us',\n",
       " 'like',\n",
       " 'kids',\n",
       " 'need',\n",
       " 'to',\n",
       " 'know',\n",
       " 'what',\n",
       " 'really',\n",
       " 'means',\n",
       " 'caps',\n",
       " '?',\n",
       " 'a',\n",
       " 'book',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'us',\n",
       " 'the',\n",
       " 'and',\n",
       " 'a',\n",
       " 'good',\n",
       " 'way',\n",
       " 'or',\n",
       " 'and',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'way',\n",
       " 'but',\n",
       " 'we',\n",
       " 'can',\n",
       " 'learned',\n",
       " 'from',\n",
       " 'it',\n",
       " ',',\n",
       " 'and',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'even',\n",
       " 'have',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'it',\n",
       " '.',\n",
       " 'movies',\n",
       " 'are',\n",
       " 'similar',\n",
       " 'like',\n",
       " 'books',\n",
       " ',',\n",
       " 'in',\n",
       " 'one',\n",
       " 'way',\n",
       " 'the',\n",
       " 'you',\n",
       " 'can',\n",
       " 'see',\n",
       " 'it',\n",
       " 'in',\n",
       " 'action',\n",
       " 'what',\n",
       " 'there',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'teach',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'some',\n",
       " 'movies',\n",
       " 'are',\n",
       " 'horrible',\n",
       " 'just',\n",
       " 'like',\n",
       " 'caps',\n",
       " 'caps',\n",
       " '.',\n",
       " 'its',\n",
       " 'a',\n",
       " 'horrible',\n",
       " 'movie',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'but',\n",
       " 'really',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'know',\n",
       " 'what',\n",
       " 'really',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'does',\n",
       " 'people',\n",
       " '.',\n",
       " 'and',\n",
       " 'even',\n",
       " 'it',\n",
       " 'a',\n",
       " 'teacher',\n",
       " 'is',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'with',\n",
       " 'your',\n",
       " 'own',\n",
       " 'eyes',\n",
       " 'to',\n",
       " 'belive',\n",
       " 'it',\n",
       " '.',\n",
       " 'a',\n",
       " 'good',\n",
       " 'way',\n",
       " 'it',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'learned',\n",
       " 'from',\n",
       " 'it',\n",
       " '.',\n",
       " 'music',\n",
       " 'has',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'being',\n",
       " 'offensive',\n",
       " 'to',\n",
       " 'people',\n",
       " 'but',\n",
       " 'singer',\n",
       " \"'s\",\n",
       " 'thats',\n",
       " 'their',\n",
       " 'way',\n",
       " 'of',\n",
       " 'express',\n",
       " 'their',\n",
       " 'own',\n",
       " 'feeling',\n",
       " 'about',\n",
       " 'stuff',\n",
       " '.',\n",
       " 'we',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'discriminate',\n",
       " 'anyone',\n",
       " 'for',\n",
       " 'their',\n",
       " 'own',\n",
       " 'belives',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'so',\n",
       " 'many',\n",
       " 'singer',\n",
       " 'like',\n",
       " 'caps',\n",
       " 'caps',\n",
       " ',',\n",
       " 'the',\n",
       " 'use',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'bad',\n",
       " 'words',\n",
       " 'in',\n",
       " 'his',\n",
       " 'song',\n",
       " 'but',\n",
       " 'thats',\n",
       " 'how',\n",
       " 'he',\n",
       " 'get',\n",
       " \"'s\",\n",
       " 'his',\n",
       " 'own',\n",
       " 'feeling',\n",
       " 'out',\n",
       " 'to',\n",
       " 'others',\n",
       " '.',\n",
       " 'even',\n",
       " 'if',\n",
       " 'the',\n",
       " 'song',\n",
       " 'is',\n",
       " 'offensive',\n",
       " 'to',\n",
       " 'you',\n",
       " 'its',\n",
       " 'more',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own',\n",
       " 'opinion',\n",
       " 'just',\n",
       " 'like',\n",
       " 'we',\n",
       " 'all',\n",
       " 'have',\n",
       " 'different',\n",
       " 'opinon',\n",
       " 'then',\n",
       " 'others',\n",
       " '.',\n",
       " 'my',\n",
       " 'opinion',\n",
       " 'about',\n",
       " 'feeling',\n",
       " 'offensive',\n",
       " 'is',\n",
       " 'we',\n",
       " 'all',\n",
       " 'are',\n",
       " 'different',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'opinion',\n",
       " 'about',\n",
       " 'it',\n",
       " 'and',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'respect',\n",
       " 'the',\n",
       " '.',\n",
       " 'especially',\n",
       " 'if',\n",
       " 'i',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'learned',\n",
       " 'new',\n",
       " 'things',\n",
       " 'and',\n",
       " 'someone',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'or',\n",
       " 'show',\n",
       " 'me',\n",
       " 'i',\n",
       " 'would',\n",
       " 'want',\n",
       " 'a',\n",
       " 'book',\n",
       " 'or',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'to',\n",
       " 'learned',\n",
       " '.',\n",
       " 'and',\n",
       " 'parents',\n",
       " 'just',\n",
       " 'becuase',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'reading',\n",
       " 'a',\n",
       " 'book',\n",
       " ',',\n",
       " 'or',\n",
       " 'watching',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'or',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'music',\n",
       " 'about',\n",
       " 'bad',\n",
       " 'stuff',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'assume',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'those',\n",
       " 'things',\n",
       " 'i',\n",
       " 'would',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'learned',\n",
       " 'about',\n",
       " 'it',\n",
       " '.',\n",
       " 'i',\n",
       " 'would',\n",
       " \"n't\",\n",
       " 'like',\n",
       " 'for',\n",
       " 'someone',\n",
       " 'to',\n",
       " 'take',\n",
       " 'a',\n",
       " 'book',\n",
       " 'off',\n",
       " 'the',\n",
       " 'shelf',\n",
       " 'especially',\n",
       " 'if',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'read',\n",
       " 'it',\n",
       " '.',\n",
       " 'remember',\n",
       " 'its',\n",
       " 'your',\n",
       " 'own',\n",
       " 'opinion',\n",
       " 'about',\n",
       " 'the',\n",
       " 'book',\n",
       " 'not',\n",
       " 'everyone',\n",
       " 'is',\n",
       " 'different',\n",
       " 'then',\n",
       " 'you']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = document_level_tokenize(X[X['essay_set']==2]['essay'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = document_level_tokenize(X[X['essay_set']==4]['essay'].iloc[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/LSTM_2_wo_corr\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.restore(sess, './models/LSTM_2_wo_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_subst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2645272 0.7354728]]\n",
      "[[0.48177874 0.51822126]]\n",
      "[[0.38591897 0.61408103]]\n",
      "[[0.43678337 0.56321663]]\n",
      "[[0.37682402 0.62317598]]\n",
      "[[0.52686536 0.47313464]]\n"
     ]
    }
   ],
   "source": [
    "for subst, i in zip(all_subst, range(3, 54, 10)):\n",
    "    tokens = document_level_tokenize(X[X['essay_set']==2]['essay'].iloc[i])\n",
    "    codes = np.array([dataset.dict.get(token, MAX_VOCAB_SIZE) for token in tokens])\n",
    "    \n",
    "    print(aes.predict(sess, [[subst[x] if x in subst.keys() else x for x in codes]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36624664 0.63375336]]\n",
      "[[0.56651306 0.43348691]]\n",
      "[[0.46504706 0.53495294]]\n",
      "[[0.50077444 0.49922556]]\n",
      "[[0.46607918 0.53392082]]\n",
      "[[0.62973565 0.37026435]]\n"
     ]
    }
   ],
   "source": [
    "for subst, i in zip(all_subst, range(3, 54, 10)):\n",
    "    tokens = document_level_tokenize(X[X['essay_set']==2]['essay'].iloc[i])\n",
    "    codes = np.array([dataset.dict.get(token, MAX_VOCAB_SIZE) for token in tokens])\n",
    "    \n",
    "#     print(aes.predict(sess, [[subst[x] if x in subst.keys() else x for x in codes]]))\n",
    "    print(aes.predict(sess, [codes]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(scores, essay_set):\n",
    "    mini = np.choose(essay_set-1,score_range_min)\n",
    "    maxi = np.choose(essay_set-1,score_range_max)\n",
    "    print(mini, maxi)\n",
    "    print(scores)\n",
    "    print(scores * (maxi - mini))\n",
    "    return np.array(np.round(scores * (maxi - mini) + mini),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6\n",
      "[0.63375336 0.43348691 0.53495294 0.49922556 0.53392082 0.37026435]\n",
      "[3.1687668  2.16743455 2.6747647  2.4961278  2.6696041  1.85132175]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, 3, 4, 3])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand(np.array([0.63375336, 0.43348691, 0.53495294, 0.49922556, 0.53392082, 0.37026435]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6\n",
      "[0.7354728  0.51822126 0.61408103 0.56321663 0.62317598 0.47313464]\n",
      "[3.677364   2.5911063  3.07040515 2.81608315 3.1158799  2.3656732 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 4, 4, 4, 4, 3])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand(np.array([0.7354728, 0.51822126, 0.61408103, 0.56321663, 0.62317598, 0.47313464]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'way'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_inv_dict[1502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paths'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_inv_dict[17092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In @DATE1's world, there are many things found offensive.  Everyone has their own opinion on what is offensive and what is not. Many parents are becoming upset because they think their children are viewing things that they should not.  Other people are upset because they think the libraries are offending their culture or ***paths*** of life.  This is even taken to the extreme where people want censhorship on libraries to avoid this, which is wrong.     Some people are becoming concerned about the materials in libraries.  They find these things to be offensive.  Everyone is entitled to their own opinion, but there really is nothing anyone can do if someone is offended.  The world is a public place and everywhere we go, something might be found offensive.  The library is a place for study.  It is never intended to offend someone, or bring bad to the world.  It is simply a place to inform, and if someone is offended by what they see, they should stay a***paths*** from the library.     I have been to the library many times, none of which have I ever seen anything offensive.  Everything I have ever witnessed at the library is for learning and research.  There are certain sections in the library.  If a parent does not want their child seeing something, they should keep their child in the children's section.  I can assure you, there is nothing offensive in the children's section, or else the library would not have it in that section.  The owners of these libraries know what is going to upset people and what will not.  If there was truly offensive materials in the library, those materials would be taken out.     Also, if a person complains, and the materials are removed, it could lessen someone else's chance getting the materials they need.  One person could think the material is offensive, but someone else might want to learn more about it.  If one is offended by a certain material, all they simply must do, is not look at it.  The library can be compared to a big computer.  One can basically find anything there.  Asking the library to censor their materials is like asking the internet to censor theirs.  It is a ***paths*** of learning and researching and it would be almost impossible to censor everything there.     Everyone is going to be offended some point in their life.  If the libraries removed everything that could offend someone, they would have no materials left.  People need to stop being so easily offended and realize the library is not trying to harm anyone.  There does not need to be any censorship in libraries.  It is simply trying to teach people about the world and let them enjoy books, music, movies, or whatever else one might go to the library to find.\""
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X['essay_set']==2]['essay'].iloc[3].replace(full_inv_dict[1502], \"***\" + full_inv_dict[17092] + \"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(20416, 17309), (21313, 3631), (28035, 730), (23236, 33548), (23941, 23600), (17985, 1535), (17481, 1160), (3808, 19279), (33614, 33563), (34064, 34167), (34065, 29541), (5012, 17512), (22296, 27245), (12314, 3901), (2016, 20409), (1502, 17092), (14751, 10366), (18016, 17510), (32034, 4111), (36771, 4722), (6308, 27423), (28199, 5144), (10281, 22024), (33447, 3747), (25391, 33169), (22322, 34412), (2590, 18806), (15606, 11715), (15226, 3808), (24699, 36181), (13430, 4796)])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subst[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In @DATE1's world, there are \\sout{ many } \\textbf{numerous} things \\sout{ found } \\textbf{detected} offensive.  Everyone has their own opinion \\sout{ on } \\textbf{concerning} what is offensive and what is not. Many parents are becoming \\sout{ upset } \\textbf{outraged} because they think their children are viewing things that they should not.  Other \\sout{ people } \\textbf{citizens} are \\sout{ upset } \\textbf{outraged} because they think the libraries are offending their culture or \\sout{ way } \\textbf{paths} of life.  This is even taken \\sout{ to } \\textbf{of} the extreme \\sout{ where } \\textbf{thus} \\sout{ people } \\textbf{citizens} want censhorship \\sout{ on } \\textbf{concerning} libraries \\sout{ to } \\textbf{of} avoid this, which is wrong.     Some \\sout{ people } \\textbf{citizens} are becoming concerned \\sout{ about } \\textbf{toward} the materials \\sout{ in } \\textbf{at} libraries.  They find these things \\sout{ to } \\textbf{of} be offensive.  Everyone is entitled \\sout{ to } \\textbf{of} their own opinion, but there really is \\sout{ nothing } \\textbf{not} anyone can do \\sout{ if } \\textbf{whether} \\sout{ someone } \\textbf{person} is offended.  The \\sout{ world } \\textbf{planet} is a public place and everywhere we go, something might be \\sout{ found } \\textbf{detected} offensive.  The \\sout{ library } \\textbf{librarians} is a place for study.  It is never intended \\sout{ to } \\textbf{of} offend someone, or bring bad \\sout{ to } \\textbf{of} the world.  It is \\sout{ simply } \\textbf{sheer} a place \\sout{ to } \\textbf{of} inform, and \\sout{ if } \\textbf{whether} \\sout{ someone } \\textbf{person} is offended by what they see, they should stay away from the library.     I have been \\sout{ to } \\textbf{of} the \\sout{ library } \\textbf{librarians} \\sout{ many } \\textbf{numerous} times, none of which have I ever seen anything offensive.  Everything I have ever witnessed at the \\sout{ library } \\textbf{librarians} is for learning and research.  There are certain sections \\sout{ in } \\textbf{at} the library.  If a parent does not want their \\sout{ child } \\textbf{childhood} \\sout{ seeing } \\textbf{viewing} something, they should keep their \\sout{ child } \\textbf{childhood} \\sout{ in } \\textbf{at} the children's section.  I can \\sout{ assure } \\textbf{ensures} you, there is \\sout{ nothing } \\textbf{not} offensive \\sout{ in } \\textbf{at} the children's section, or else the \\sout{ library } \\textbf{librarians} \\sout{ would } \\textbf{ought} not have it \\sout{ in } \\textbf{at} that section.  The owners of these libraries know what is going \\sout{ to } \\textbf{of} \\sout{ upset } \\textbf{outraged} \\sout{ people } \\textbf{citizens} and what will not.  If there was truly offensive materials \\sout{ in } \\textbf{at} the library, those materials \\sout{ would } \\textbf{ought} be taken out.     Also, \\sout{ if } \\textbf{whether} a person complains, and the materials are removed, it \\sout{ could } \\textbf{would} lessen \\sout{ someone } \\textbf{person} else's chance getting the materials they need.  One person \\sout{ could } \\textbf{would} think the material is offensive, but \\sout{ someone } \\textbf{person} else might want \\sout{ to } \\textbf{of} learn more \\sout{ about } \\textbf{toward} it.  If one is offended by a certain material, all they \\sout{ simply } \\textbf{sheer} must do, is not look at it.  The \\sout{ library } \\textbf{librarians} can be compared \\sout{ to } \\textbf{of} a big computer.  One can basically find anything there.  Asking the \\sout{ library } \\textbf{librarians} \\sout{ to } \\textbf{of} censor their materials is like asking the internet \\sout{ to } \\textbf{of} censor theirs.  It is a \\sout{ way } \\textbf{paths} of learning and researching and it \\sout{ would } \\textbf{ought} be almost impossible \\sout{ to } \\textbf{of} censor everything there.     Everyone is going \\sout{ to } \\textbf{of} be offended some point \\sout{ in } \\textbf{at} their life.  If the libraries \\sout{ removed } \\textbf{eradicated} everything that \\sout{ could } \\textbf{would} offend someone, they \\sout{ would } \\textbf{ought} have \\sout{ no } \\textbf{neither} materials left.  People need \\sout{ to } \\textbf{of} stop being so easily offended and \\sout{ realize } \\textbf{reaching} the \\sout{ library } \\textbf{librarians} is not \\sout{ trying } \\textbf{striving} \\sout{ to } \\textbf{of} harm anyone.  There does not need \\sout{ to } \\textbf{of} be any censorship \\sout{ in } \\textbf{at} libraries.  It is \\sout{ simply } \\textbf{sheer} \\sout{ trying } \\textbf{striving} \\sout{ to } \\textbf{of} teach \\sout{ people } \\textbf{citizens} \\sout{ about } \\textbf{toward} the \\sout{ world } \\textbf{planet} and let them enjoy books, music, movies, or whatever else one might go \\sout{ to } \\textbf{of} the \\sout{ library } \\textbf{librarians} \\sout{ to } \\textbf{of} find.\n",
      "\n",
      "\n",
      "\n",
      "Yes and no, some materials such as books, music, movies, magazines, etc, \\sout{ should } \\textbf{must} be \\sout{ voted } \\textbf{adopted} upon the citizens \\sout{ to } \\textbf{of} be removed from shelves.          I do think that some materials in those catagories \\sout{ should } \\textbf{must} be removed \\sout{ if } \\textbf{whether} they are \\sout{ offensive } \\textbf{abusive} \\sout{ to } \\textbf{of} me as well as others, but it will take a long while \\sout{ to } \\textbf{of} get them removed from stores and other places \\sout{ if } \\textbf{whether} other people \\sout{ like } \\textbf{fond} them. I do not \\sout{ like } \\textbf{fond} how they make some music \\sout{ to } \\textbf{of} be very violent and \\sout{ cause } \\textbf{provoke} \\sout{ minds } \\textbf{souls} of most teenagers \\sout{ to } \\textbf{of} \\sout{ turn } \\textbf{transform} \\sout{ bad } \\textbf{wicked} and start selling \\sout{ drugs } \\textbf{drug} on the street of their hometown, but i can't do anything about that because that kind of music is admired by those teenagers as well as some adults too. If some people can buckle down and see that stuff \\sout{ like } \\textbf{fond} that will \\sout{ mess } \\textbf{chaos} up lives of teenagers and some adults who fall victim \\sout{ to } \\textbf{of} it, then there is a chance that it can be stopped. Stopping \\sout{ things } \\textbf{elements} \\sout{ like } \\textbf{fond} this will \\sout{ save } \\textbf{rescue} a community from disaster and \\sout{ cause } \\textbf{provoke} other good chances in life \\sout{ for } \\textbf{into} people in need \\sout{ for } \\textbf{into} those chances.     Here's \\sout{ another } \\textbf{further} example, \\sout{ like } \\textbf{fond} this music artist named @CAPS1 @CAPS2. She has made some \\sout{ great } \\textbf{wonderful} songs \\sout{ for } \\textbf{into} the past year and a half now. People have \\sout{ told } \\textbf{say} me that she is part of a group called @CAPS3 and its a group \\sout{ where } \\textbf{hence} they try \\sout{ to } \\textbf{of} I think 're-birth' thereselves. My \\sout{ friends } \\textbf{boyfriends} wanted me \\sout{ to } \\textbf{of} stop listening \\sout{ to } \\textbf{of} her music. I \\sout{ told } \\textbf{say} my \\sout{ friends } \\textbf{boyfriends} that I do not \\sout{ like } \\textbf{fond} the fact that she joined this group, but that doesnt mean im gonna stop listen \\sout{ to } \\textbf{of} her music. Now \\sout{ if } \\textbf{whether} she makes a song that is \\sout{ offensive } \\textbf{abusive} \\sout{ to } \\textbf{of} me and as well as my friends, then that \\sout{ where } \\textbf{hence} I draw the line.     What im saying is that \\sout{ if } \\textbf{whether} people don't have others \\sout{ to } \\textbf{of} back them up \\sout{ if } \\textbf{whether} something is highly \\sout{ offensive } \\textbf{abusive} \\sout{ to } \\textbf{of} them and oblivious \\sout{ to } \\textbf{of} others, it will be very hard trying \\sout{ to } \\textbf{of} prove yourself in the best \\sout{ way } \\textbf{manner} possible.\n",
      "\n",
      "\n",
      "\n",
      "Books Removed From Shelves?      'All of us can \\sout{ think } \\textbf{ideas} of a book that we hope none of our \\sout{ children } \\textbf{childhood} \\sout{ or } \\textbf{neither} any other \\sout{ children } \\textbf{childhood} have taken off the chelf.  But if I have the right \\sout{ to } \\textbf{of} remove that book from the shelf- that I work I abhor- then you also have exactly the same right and so does everyone else.  And then we have no nooks left on the shelf \\sout{ for } \\textbf{into} any of us.' @CAPS1 statement was from @PERSON1.  Do you \\sout{ think } \\textbf{ideas} it is right \\sout{ for } \\textbf{into} young \\sout{ children } \\textbf{childhood} \\sout{ to } \\textbf{of} look at immages that are \\sout{ not } \\textbf{neither} appropiate \\sout{ or } \\textbf{neither} read \\sout{ something } \\textbf{nothing} that is talking baout sexual content?  I do \\sout{ not } \\textbf{neither} \\sout{ think } \\textbf{ideas} any of these materials \\sout{ should } \\textbf{ought} be removed from the shelves, \\sout{ but } \\textbf{yet} they \\sout{ should } \\textbf{ought} be put in a privite \\sout{ area } \\textbf{realm} \\sout{ where } \\textbf{thus} \\sout{ only } \\textbf{merely} adults can get \\sout{ to } \\textbf{of} them.       I \\sout{ think } \\textbf{ideas} they \\sout{ should } \\textbf{ought} be put in a certain area, away from children, because \\sout{ children } \\textbf{childhood} will get ideas from the material that they see and \\sout{ think } \\textbf{ideas} it is okay \\sout{ to } \\textbf{of} act like that.  There are some books and \\sout{ movies } \\textbf{films} \\sout{ out } \\textbf{outward} there that \\sout{ teenagers } \\textbf{adolescent} and young \\sout{ children } \\textbf{childhood} see that are about \\sout{ cereal } \\textbf{grains} killers and \\sout{ children } \\textbf{childhood} \\sout{ think } \\textbf{ideas} it is okay \\sout{ to } \\textbf{of} bring \\sout{ guns } \\textbf{weapons} \\sout{ to } \\textbf{of} school and it is \\sout{ not } \\textbf{neither} okay.  Children are reading these books and thinking it is okay \\sout{ to } \\textbf{of} act like their favorite character \\sout{ or } \\textbf{neither} acting like their favorite movie star in a movie.  Some of the \\sout{ movies } \\textbf{films} that are \\sout{ out } \\textbf{outward} there are about doing stupid stuff, and \\sout{ children } \\textbf{childhood} and \\sout{ teenagers } \\textbf{adolescent} \\sout{ think } \\textbf{ideas} it is 'cool' because the person that they look up \\sout{ to } \\textbf{of} is doing that, so they \\sout{ want } \\textbf{wanting} \\sout{ to } \\textbf{of} \\sout{ try } \\textbf{attempt} it \\sout{ out } \\textbf{outward} also.  Children get hurt from doing @CAPS1.  Whenever a \\sout{ child } \\textbf{childhood} sees people having sex, they \\sout{ think } \\textbf{ideas} it is okay \\sout{ to } \\textbf{of} do it too, becasue those kids in the \\sout{ movies } \\textbf{films} are the same age as they are.   I \\sout{ think } \\textbf{ideas} that movie producers shoudl be aware of what they put \\sout{ out } \\textbf{outward} there \\sout{ for } \\textbf{into} \\sout{ children } \\textbf{childhood} \\sout{ to } \\textbf{of} watch.       When you talk about the book @CAPS2 @CAPS3, you \\sout{ think } \\textbf{ideas} what a great book.  But would you ever \\sout{ think } \\textbf{ideas} that it would be taken off of \\sout{ shelves } \\textbf{bookshelves} because of one word that was used in it?  I \\sout{ think } \\textbf{ideas} that it \\sout{ should } \\textbf{ought} stay on the shelves, \\sout{ but } \\textbf{yet} if a parent does \\sout{ not } \\textbf{neither} \\sout{ want } \\textbf{wanting} their \\sout{ child } \\textbf{childhood} \\sout{ to } \\textbf{of} read it, then they do \\sout{ not } \\textbf{neither} have \\sout{ to } \\textbf{of} read it.  If parents do \\sout{ not } \\textbf{neither} \\sout{ want } \\textbf{wanting} their \\sout{ child } \\textbf{childhood} \\sout{ to } \\textbf{of} read \\sout{ something } \\textbf{nothing} then just tell the teacher \\sout{ or } \\textbf{neither} \\sout{ child } \\textbf{childhood} that they cannot read the book.  I \\sout{ think } \\textbf{ideas} the books \\sout{ should } \\textbf{ought} stay on the \\sout{ shelves } \\textbf{bookshelves} and any material that has any 'bad' sontent on it \\sout{ should } \\textbf{ought} be put in a private section \\sout{ where } \\textbf{thus} \\sout{ only } \\textbf{merely} adults can get \\sout{ to } \\textbf{of} it.       Do you \\sout{ want } \\textbf{wanting} your \\sout{ children } \\textbf{childhood} looking at bad pictures \\sout{ or } \\textbf{neither} seeing \\sout{ or } \\textbf{neither} reading about \\sout{ something } \\textbf{nothing} with sexual content?  Children are getting bad ideas from books, movies, magazines, and music.  They \\sout{ think } \\textbf{ideas} it is appropiate \\sout{ to } \\textbf{of} act like @CAPS1, because their favotire movie star is preforming @CAPS1 way.  These books, movies, and magazines need \\sout{ to } \\textbf{of} be put up \\sout{ out } \\textbf{outward} of children's reach, so they cannot get anymore bad ideas \\sout{ to } \\textbf{of} do stupid stuff\n",
      "\n",
      "\n",
      "\n",
      "Would \\sout{ you } \\textbf{thou} want your childern reaing about \\sout{ things } \\textbf{subjects} that only @CAPS1 know's what? When \\sout{ you } \\textbf{thou} go \\sout{ to } \\textbf{of} a library \\sout{ you } \\textbf{thou} aspect \\sout{ to } \\textbf{of} learn about @CAPS2, @CAPS3, @LOCATION1's @CAPS4,@CAPS5 etc. Libraries \\sout{ are } \\textbf{constitute} for learning new \\sout{ things } \\textbf{subjects} about the world that will \\sout{ later } \\textbf{subsequently} \\sout{ help } \\textbf{helps} \\sout{ you } \\textbf{thou} in @CAPS9. When \\sout{ you } \\textbf{thou} first walk into a library \\sout{ you } \\textbf{thou} except \\sout{ to } \\textbf{of} see people checking out @CAPS2 books , or books that catch your eye \\sout{ just } \\textbf{merely} by the title. If we find a book is offensive , or will not \\sout{ help } \\textbf{helps} better our childerns' future \\sout{ then } \\textbf{upon} stand up and fight for their own  mental development.     We must \\sout{ also } \\textbf{likewise} think of what the childern want \\sout{ to } \\textbf{of} read. They have the right \\sout{ to } \\textbf{of} read what ever they want, as long as it's \\sout{ entertaining } \\textbf{hilarious} \\sout{ to } \\textbf{of} them and they \\sout{ are } \\textbf{constitute} learning something new. Some books teaches them about the world they \\sout{ are } \\textbf{constitute} growing up in. There \\sout{ are } \\textbf{constitute} some books that I would not let my own \\sout{ child } \\textbf{childhood} read, but I know in my heart that she is learning something that I @MONTH1 not be able \\sout{ to } \\textbf{of} teach her. Those types of books of \\sout{ are } \\textbf{constitute} what I call, '@CAPS7's'. Those books that can come off seeming offensive, when in the end they \\sout{ are } \\textbf{constitute} actually, what I \\sout{ call } \\textbf{appealed} '@CAPS8', helping \\sout{ to } \\textbf{of} \\sout{ prepare } \\textbf{elaborate} them for what is it come. Not every book will be full of rainbows, \\sout{ pretty } \\textbf{rather} colors, or pop-ups. They must know that they \\sout{ are } \\textbf{constitute} some people they have \\sout{ to } \\textbf{of} be mindfull of, and people who \\sout{ are } \\textbf{constitute} educating them on @CAPS9. They have \\sout{ to } \\textbf{of} learn the difference between what's right, and what's wrong.          Remember the first book \\sout{ you } \\textbf{thou} ever read by yourself? I do. It was called 'Of @CAPS10 and @CAPS11'. I read that book when I was @NUM1. Till this day my mother says, 'I tried \\sout{ to } \\textbf{of} stop \\sout{ you } \\textbf{thou} from reading that book \\sout{ so } \\textbf{therefore} many times , it \\sout{ had } \\textbf{has} \\sout{ dangerous } \\textbf{dangers} wording that an @NUM1 year \\sout{ should } \\textbf{ought} no be able \\sout{ to } \\textbf{of} read at that young age'. What she did not know was that; that book \\sout{ had } \\textbf{has} taught me alot about the world back then. That knowledge I \\sout{ had } \\textbf{has} obtain \\sout{ then } \\textbf{upon} \\sout{ had } \\textbf{has} helped me \\sout{ later } \\textbf{subsequently} on my @CAPS9.           Some books \\sout{ are } \\textbf{constitute} ment \\sout{ to } \\textbf{of} be read while some aren't. If \\sout{ you } \\textbf{thou} feel your \\sout{ child } \\textbf{childhood} \\sout{ should } \\textbf{ought} not read a certain book \\sout{ then } \\textbf{upon} read it for yourself, and \\sout{ then } \\textbf{upon} tell your \\sout{ child } \\textbf{childhood} the reason why they can not read the same book \\sout{ you } \\textbf{thou} \\sout{ had } \\textbf{has} \\sout{ just } \\textbf{merely} read.\n",
      "\n",
      "\n",
      "\n",
      "In this millenium we really should'nt be to concerned about censorhip. People say \\sout{ so } \\textbf{thus} way worse \\sout{ in } \\textbf{among} puplic and \\sout{ do } \\textbf{does} worse things \\sout{ in } \\textbf{among} public. On \\sout{ the } \\textbf{to} other side of this, \\sout{ the } \\textbf{to} \\sout{ parents } \\textbf{parental} of a \\sout{ child } \\textbf{childhood} \\sout{ should } \\textbf{ought} be watching their \\sout{ kid } \\textbf{infantile} and looking to see what they \\sout{ are } \\textbf{constitute} about to grab off \\sout{ the } \\textbf{to} shelf. Its \\sout{ the } \\textbf{to} \\sout{ parents } \\textbf{parental} fault \\sout{ if } \\textbf{whether} their \\sout{ child } \\textbf{childhood} gets ahold of a \\sout{ book } \\textbf{cookbook} that has \\sout{ adult } \\textbf{adulthood} content on or \\sout{ in } \\textbf{among} it. So why \\sout{ should } \\textbf{ought} we have censorship?                                                                             If a cd contains \\sout{ adult } \\textbf{adulthood} content and \\sout{ the } \\textbf{to} \\sout{ child } \\textbf{childhood} is \\sout{ underage } \\textbf{youngsters} and grabs it off \\sout{ the } \\textbf{to} shelf listening to it, \\sout{ then } \\textbf{upon} its \\sout{ the } \\textbf{to} \\sout{ parents } \\textbf{parental} fault for \\sout{ not } \\textbf{neither} \\sout{ being } \\textbf{ongoing} responsible enough to watch their child. Same goes for books and etc.. Now \\sout{ if } \\textbf{whether} \\sout{ the } \\textbf{to} \\sout{ child } \\textbf{childhood} is old enough to understand what they grab \\sout{ then } \\textbf{upon} they \\sout{ should } \\textbf{ought} be able to listen to it, and ask questions about it. If its about \\sout{ adult } \\textbf{adulthood} relationships and \\sout{ the } \\textbf{to} \\sout{ child } \\textbf{childhood} takes it, \\sout{ then } \\textbf{upon} \\sout{ the } \\textbf{to} parent \\sout{ should } \\textbf{ought} explain it all and tell them what is going on. The parent \\sout{ should } \\textbf{ought} be \\sout{ the } \\textbf{to} one to take responsibility.                           Now, \\sout{ if } \\textbf{whether} a book, music, movie, magazine, and ect.contain raceism \\sout{ in } \\textbf{among} it \\sout{ then } \\textbf{upon} it \\sout{ should } \\textbf{ought} be censored, unless its about history. Kids under \\sout{ the } \\textbf{to} age of teen usally repeat what they hear or read. Same goes for \\sout{ teenagers } \\textbf{teenage} too actualy! Books with \\sout{ the } \\textbf{to} \\sout{ wrong } \\textbf{misguided} kind of words \\sout{ in } \\textbf{among} it \\sout{ should } \\textbf{ought} be kept safe or there \\sout{ should } \\textbf{ought} be a age to where you can get it. The world has enough horrid language \\sout{ in } \\textbf{among} it, why \\sout{ should } \\textbf{ought} \\sout{ the } \\textbf{to} \\sout{ library } \\textbf{librarians} add to it by \\sout{ letting } \\textbf{allowing} \\sout{ kids } \\textbf{youngsters} get ahold of these books and repeating what they hear.                  I remember when I \\sout{ was } \\textbf{became} walking around a \\sout{ library } \\textbf{librarians} and I \\sout{ saw } \\textbf{witnessed} a little \\sout{ kid } \\textbf{infantile} \\sout{ not } \\textbf{neither} \\sout{ older } \\textbf{age} than @NUM1 grab an \\sout{ adult } \\textbf{adulthood} \\sout{ book } \\textbf{cookbook} and \\sout{ started } \\textbf{begun} to open it and look through it. The parent \\sout{ was } \\textbf{became} \\sout{ no } \\textbf{neither} where to be \\sout{ seen } \\textbf{regarded} \\sout{ nor } \\textbf{neither} heard, who leaves their \\sout{ child } \\textbf{childhood} \\sout{ in } \\textbf{among} a \\sout{ book } \\textbf{cookbook} section like that? This young boy put \\sout{ the } \\textbf{to} \\sout{ book } \\textbf{cookbook} down on \\sout{ the } \\textbf{to} \\sout{ hard } \\textbf{harsh} floor and \\sout{ started } \\textbf{begun} to take off his clothes. This \\sout{ was } \\textbf{became} \\sout{ no } \\textbf{neither} ones fault, \\sout{ wasnt } \\textbf{didnt} \\sout{ the } \\textbf{to} library, \\sout{ wasnt } \\textbf{didnt} mine. People \\sout{ should } \\textbf{ought} take responsibility, control their \\sout{ kids } \\textbf{youngsters} and \\sout{ if } \\textbf{whether} they can't \\sout{ then } \\textbf{upon} leave them \\sout{ at } \\textbf{under} home!                                                        There \\sout{ are } \\textbf{constitute} also certain people who need \\sout{ book } \\textbf{cookbook} for research and need to take home books for their projects. We should'nt have censorship and take down all \\sout{ the } \\textbf{to} books that \\sout{ should } \\textbf{ought} be contained from kids. Some of us need those kind of books \\sout{ in } \\textbf{among} order for us to \\sout{ do } \\textbf{does} our research and projects!      No we should'nt have censorship \\sout{ in } \\textbf{among} our libraries! Im \\sout{ not } \\textbf{neither} about to be able to miss a project or miss a good movie because some \\sout{ parents } \\textbf{parental} don't know how to control their \\sout{ kids } \\textbf{youngsters} and watch what they grab. We \\sout{ should } \\textbf{ought} be able to keep all \\sout{ the } \\textbf{to} books, movies, magazines, and ect., there could be a way to censor most books but they would need their own shelf, and they could be a age lemit or a pass people could use. That would take to long to \\sout{ do } \\textbf{does} \\sout{ so } \\textbf{thus} though. So keep our libraries filled with interesting things without censoring them!\n",
      "\n",
      "\n",
      "\n",
      "Every \\sout{ library } \\textbf{libraries} has tons of \\sout{ information } \\textbf{detail} on just \\sout{ about } \\textbf{nearly} everything.  There \\sout{ are } \\textbf{constitute} tons of books, music, movies, magazines and more that contain this information.  There \\sout{ is } \\textbf{exists} even some \\sout{ information } \\textbf{detail} that some \\sout{ people } \\textbf{citizens} \\sout{ might } \\textbf{perhaps} take \\sout{ offence } \\textbf{violations} to.  On the other hand, somebody else \\sout{ could } \\textbf{would} be perfectly fine with it.  I think that \\sout{ all } \\textbf{entire} \\sout{ libraries } \\textbf{librarians} should keep \\sout{ all } \\textbf{entire} their books on the shelfs, no matter what \\sout{ kind } \\textbf{genre} of content they contain, because not everybody takes \\sout{ offence } \\textbf{violations} \\sout{ to } \\textbf{of} some contents in books, and it \\sout{ is } \\textbf{exists} the same \\sout{ way } \\textbf{paths} vice versa.     The subject \\sout{ race } \\textbf{ethnicity} \\sout{ might } \\textbf{perhaps} come into play in some books in libraries.  Books \\sout{ have } \\textbf{has} \\sout{ information } \\textbf{detail} \\sout{ about } \\textbf{nearly} \\sout{ people } \\textbf{citizens} doing \\sout{ things } \\textbf{questions} \\sout{ to } \\textbf{of} other \\sout{ people } \\textbf{citizens} just because of their race.  Some \\sout{ people } \\textbf{citizens} want books took out of their \\sout{ libraries } \\textbf{librarians} because of the content that they contain, but really it \\sout{ could } \\textbf{would} be a n \\sout{ important } \\textbf{vital} \\sout{ part } \\textbf{parties} of history that \\sout{ people } \\textbf{citizens} need \\sout{ to } \\textbf{of} be educated about.  Some books \\sout{ have } \\textbf{has} words that \\sout{ are } \\textbf{constitute} \\sout{ very } \\textbf{highly} offencive \\sout{ to } \\textbf{of} \\sout{ people } \\textbf{citizens} because of their skin color, relgion, or ethnic race.  This \\sout{ information } \\textbf{detail} that \\sout{ people } \\textbf{citizens} need \\sout{ to } \\textbf{of} know because \\sout{ thats } \\textbf{cant} how \\sout{ people } \\textbf{citizens} were treated back then and some stuff like that still occurs today.     Libraries \\sout{ have } \\textbf{has} \\sout{ all } \\textbf{entire} \\sout{ kinds } \\textbf{sorts} of information.  I think that \\sout{ all } \\textbf{entire} the books at \\sout{ libraries } \\textbf{librarians} should stay there no matter what they contain.  Even if some content \\sout{ is } \\textbf{exists} offensive some \\sout{ people } \\textbf{citizens} it \\sout{ could } \\textbf{would} still be a \\sout{ important } \\textbf{vital} \\sout{ part } \\textbf{parties} of history that \\sout{ people } \\textbf{citizens} need \\sout{ to } \\textbf{of} know\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ess_ex in range(6):\n",
    "    t = X[X['essay_set']==2]['essay'].iloc[3+ess_ex*10]\n",
    "    sub = all_subst[ess_ex]\n",
    "    for k,v in sub.items():\n",
    "        t = t.replace(\" \" + full_inv_dict[k] + \" \", \" \\\\sout{ \" +full_inv_dict[k] + \" } \\\\textbf{\" + full_inv_dict[v] + \"} \")\n",
    "    \n",
    "    print(t)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1502: 17092,\n",
       " 2016: 20409,\n",
       " 2590: 18806,\n",
       " 3808: 19279,\n",
       " 5012: 17512,\n",
       " 6308: 27423,\n",
       " 10281: 22024,\n",
       " 12314: 3901,\n",
       " 13430: 4796,\n",
       " 14751: 10366,\n",
       " 15226: 3808,\n",
       " 15606: 11715,\n",
       " 17481: 1160,\n",
       " 17985: 1535,\n",
       " 18016: 17510,\n",
       " 20416: 17309,\n",
       " 21313: 3631,\n",
       " 22296: 27245,\n",
       " 22322: 34412,\n",
       " 23236: 33548,\n",
       " 23941: 23600,\n",
       " 24699: 36181,\n",
       " 25391: 33169,\n",
       " 28035: 730,\n",
       " 28199: 5144,\n",
       " 32034: 4111,\n",
       " 33447: 3747,\n",
       " 33614: 33563,\n",
       " 34064: 34167,\n",
       " 34065: 29541,\n",
       " 36771: 4722}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/LSTM_2_wo_corr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.6363882422447205\n",
      "\t\t 1  --  0.6366785764694214\n",
      "\t\t 2  --  0.6386632323265076\n",
      "\t\t 3  --  0.6407425999641418\n",
      "\t\t 4  --  0.6417069435119629\n",
      "\t\t 5  --  0.6464125514030457\n",
      "\t\t 6  --  0.6498331427574158\n",
      "\t\t 7  --  0.6519977450370789\n",
      "\t\t 8  --  0.6549655199050903\n",
      "\t\t 9  --  0.6596347093582153\n",
      "\t\t 10  --  0.6596347093582153\n",
      "\t\t 11  --  0.6596347093582153\n",
      "\t\t 12  --  0.6596347093582153\n",
      "\t\t 13  --  0.6596347093582153\n",
      "\t\t 14  --  0.6636547446250916\n",
      "\t\t 15  --  0.6636547446250916\n",
      "\t\t 16  --  0.6636547446250916\n",
      "\t\t 17  --  0.6652864813804626\n",
      "\t\t 18  --  0.6652864813804626\n",
      "\t\t 19  --  0.6690306067466736\n",
      "\t\t 20  --  0.6690306067466736\n",
      "\t\t 21  --  0.672046422958374\n",
      "\t\t 22  --  0.672046422958374\n",
      "\t\t 23  --  0.6768859624862671\n",
      "\t\t 24  --  0.6790741086006165\n",
      "\t\t 25  --  0.6790741086006165\n",
      "\t\t 26  --  0.6790741086006165\n",
      "\t\t 27  --  0.6790741086006165\n",
      "\t\t 28  --  0.6790741086006165\n",
      "\t\t 29  --  0.6790741086006165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [18:10<2:43:34, 1090.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.4363124668598175\n",
      "\t\t 1  --  0.4390597939491272\n",
      "\t\t 2  --  0.4407338798046112\n",
      "\t\t 3  --  0.4442596137523651\n",
      "\t\t 4  --  0.44442513585090637\n",
      "\t\t 5  --  0.4490482807159424\n",
      "\t\t 6  --  0.4490482807159424\n",
      "\t\t 7  --  0.45010796189308167\n",
      "\t\t 8  --  0.45062142610549927\n",
      "\t\t 9  --  0.4580678641796112\n",
      "\t\t 10  --  0.4580678641796112\n",
      "\t\t 11  --  0.4580678641796112\n",
      "\t\t 12  --  0.4580678641796112\n",
      "\t\t 13  --  0.45983806252479553\n",
      "\t\t 14  --  0.46097180247306824\n",
      "\t\t 15  --  0.46097180247306824\n",
      "\t\t 16  --  0.46097180247306824\n",
      "\t\t 17  --  0.4781911075115204\n",
      "\t\t 18  --  0.4801833927631378\n",
      "\t\t 19  --  0.4801833927631378\n",
      "\t\t 20  --  0.4866497218608856\n",
      "\t\t 21  --  0.4866497218608856\n",
      "\t\t 22  --  0.4866497218608856\n",
      "\t\t 23  --  0.4866497218608856\n",
      "\t\t 24  --  0.4866497218608856\n",
      "\t\t 25  --  0.4866497218608856\n",
      "\t\t 26  --  0.49075862765312195\n",
      "\t\t 27  --  0.49075862765312195\n",
      "\t\t 28  --  0.49075862765312195\n",
      "\t\t 29  --  0.49075862765312195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [36:50<2:26:34, 1099.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.5365798473358154\n",
      "\t\t 1  --  0.5381683707237244\n",
      "\t\t 2  --  0.5381683707237244\n",
      "\t\t 3  --  0.541584312915802\n",
      "\t\t 4  --  0.5421772599220276\n",
      "\t\t 5  --  0.5440228581428528\n",
      "\t\t 6  --  0.5474314093589783\n",
      "\t\t 7  --  0.5474314093589783\n",
      "\t\t 8  --  0.548632800579071\n",
      "\t\t 9  --  0.5525274872779846\n",
      "\t\t 10  --  0.5525274872779846\n",
      "\t\t 11  --  0.5532901287078857\n",
      "\t\t 12  --  0.5546805262565613\n",
      "\t\t 13  --  0.5546805262565613\n",
      "\t\t 14  --  0.557384192943573\n",
      "\t\t 15  --  0.5574179291725159\n",
      "\t\t 16  --  0.5574179291725159\n",
      "\t\t 17  --  0.5574179291725159\n",
      "\t\t 18  --  0.5574179291725159\n",
      "\t\t 19  --  0.55979323387146\n",
      "\t\t 20  --  0.5613972544670105\n",
      "\t\t 21  --  0.5625898838043213\n",
      "\t\t 22  --  0.5625898838043213\n",
      "\t\t 23  --  0.5625898838043213\n",
      "\t\t 24  --  0.5631833672523499\n",
      "\t\t 25  --  0.5657237768173218\n",
      "\t\t 26  --  0.5657237768173218\n",
      "\t\t 27  --  0.5680223703384399\n",
      "\t\t 28  --  0.5680223703384399\n",
      "\t\t 29  --  0.5691534280776978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [55:33<2:09:05, 1106.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.5012348294258118\n",
      "\t\t 1  --  0.5027823448181152\n",
      "\t\t 2  --  0.5057712197303772\n",
      "\t\t 3  --  0.5065935254096985\n",
      "\t\t 4  --  0.5090824365615845\n",
      "\t\t 5  --  0.5114927887916565\n",
      "\t\t 6  --  0.5114927887916565\n",
      "\t\t 7  --  0.5143865346908569\n",
      "\t\t 8  --  0.5143865346908569\n",
      "\t\t 9  --  0.5143865346908569\n",
      "\t\t 10  --  0.5143865346908569\n",
      "\t\t 11  --  0.5143865346908569\n",
      "\t\t 12  --  0.5147401094436646\n",
      "\t\t 13  --  0.5147401094436646\n",
      "\t\t 14  --  0.5155943036079407\n",
      "\t\t 15  --  0.5155943036079407\n",
      "\t\t 16  --  0.5155943036079407\n",
      "\t\t 17  --  0.5167288184165955\n",
      "\t\t 18  --  0.5172983407974243\n",
      "\t\t 19  --  0.5172983407974243\n",
      "\t\t 20  --  0.5179058313369751\n",
      "\t\t 21  --  0.5183959007263184\n",
      "\t\t 22  --  0.5187830924987793\n",
      "\t\t 23  --  0.5199525952339172\n",
      "\t\t 24  --  0.5228862762451172\n",
      "\t\t 25  --  0.5228862762451172\n",
      "\t\t 26  --  0.5243529081344604\n",
      "\t\t 27  --  0.5274035930633545\n",
      "\t\t 28  --  0.5312598347663879\n",
      "\t\t 29  --  0.5312598347663879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [1:14:07<1:50:52, 1108.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.5357636213302612\n",
      "\t\t 1  --  0.5362395644187927\n",
      "\t\t 2  --  0.5375198125839233\n",
      "\t\t 3  --  0.5388099551200867\n",
      "\t\t 4  --  0.5390035510063171\n",
      "\t\t 5  --  0.5408312678337097\n",
      "\t\t 6  --  0.5414062142372131\n",
      "\t\t 7  --  0.5432431101799011\n",
      "\t\t 8  --  0.5439964532852173\n",
      "\t\t 9  --  0.5452370643615723\n",
      "\t\t 10  --  0.5480722784996033\n",
      "\t\t 11  --  0.5480722784996033\n",
      "\t\t 12  --  0.5480722784996033\n",
      "\t\t 13  --  0.5480722784996033\n",
      "\t\t 14  --  0.5532879829406738\n",
      "\t\t 15  --  0.5532879829406738\n",
      "\t\t 16  --  0.5536571145057678\n",
      "\t\t 17  --  0.5540415644645691\n",
      "\t\t 18  --  0.5540415644645691\n",
      "\t\t 19  --  0.5577232241630554\n",
      "\t\t 20  --  0.5577232241630554\n",
      "\t\t 21  --  0.5615342855453491\n",
      "\t\t 22  --  0.5615342855453491\n",
      "\t\t 23  --  0.5615342855453491\n",
      "\t\t 24  --  0.5646524429321289\n",
      "\t\t 25  --  0.5646524429321289\n",
      "\t\t 26  --  0.5646524429321289\n",
      "\t\t 27  --  0.5665755867958069\n",
      "\t\t 28  --  0.5689210891723633\n",
      "\t\t 29  --  0.5707806348800659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [1:33:01<1:33:01, 1116.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.372163861989975\n",
      "\t\t 1  --  0.3730548918247223\n",
      "\t\t 2  --  0.37573227286338806\n",
      "\t\t 3  --  0.37573227286338806\n",
      "\t\t 4  --  0.3794148564338684\n",
      "\t\t 5  --  0.3794148564338684\n",
      "\t\t 6  --  0.3798307776451111\n",
      "\t\t 7  --  0.38149121403694153\n",
      "\t\t 8  --  0.4204258918762207\n",
      "\t\t 9  --  0.4227345287799835\n",
      "\t\t 10  --  0.42320820689201355\n",
      "\t\t 11  --  0.42320820689201355\n",
      "\t\t 12  --  0.42559748888015747\n",
      "\t\t 13  --  0.42882856726646423\n",
      "\t\t 14  --  0.42882856726646423\n",
      "\t\t 15  --  0.42882856726646423\n",
      "\t\t 16  --  0.42882856726646423\n",
      "\t\t 17  --  0.42882856726646423\n",
      "\t\t 18  --  0.42904454469680786\n",
      "\t\t 19  --  0.42955484986305237\n",
      "\t\t 20  --  0.42962929606437683\n",
      "\t\t 21  --  0.4311886429786682\n",
      "\t\t 22  --  0.4311886429786682\n",
      "\t\t 23  --  0.4344063103199005\n",
      "\t\t 24  --  0.437075674533844\n",
      "\t\t 25  --  0.4407430589199066\n",
      "\t\t 26  --  0.4407430589199066\n",
      "\t\t 27  --  0.4407430589199066\n",
      "\t\t 28  --  0.4424087703227997\n",
      "\t\t 29  --  0.446952760219574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [1:51:26<1:14:11, 1112.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.5286687612533569\n",
      "\t\t 1  --  0.5312744975090027\n",
      "\t\t 2  --  0.5312744975090027\n",
      "\t\t 3  --  0.5348475575447083\n",
      "\t\t 4  --  0.5378063917160034\n",
      "\t\t 5  --  0.5389983654022217\n",
      "\t\t 6  --  0.5413410663604736\n",
      "\t\t 7  --  0.5413410663604736\n",
      "\t\t 8  --  0.5413410663604736\n",
      "\t\t 9  --  0.5416242480278015\n",
      "\t\t 10  --  0.5426993370056152\n",
      "\t\t 11  --  0.5439085364341736\n",
      "\t\t 12  --  0.5449575185775757\n",
      "\t\t 13  --  0.5455754399299622\n",
      "\t\t 14  --  0.5483925342559814\n",
      "\t\t 15  --  0.5483925342559814\n",
      "\t\t 16  --  0.5483925342559814\n",
      "\t\t 17  --  0.5513394474983215\n",
      "\t\t 18  --  0.5559113025665283\n",
      "\t\t 19  --  0.5559113025665283\n",
      "\t\t 20  --  0.5559113025665283\n",
      "\t\t 21  --  0.5559113025665283\n",
      "\t\t 22  --  0.5575400590896606\n",
      "\t\t 23  --  0.5575400590896606\n",
      "\t\t 24  --  0.5575400590896606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-9307485fbf51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_VOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mga_attack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msubst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/essaysense/nlp_adversarial_examples/attacks.py\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, x_orig, target, max_change)\u001b[0m\n\u001b[1;32m    153\u001b[0m                                      pop[parent2_idx[i]])\n\u001b[1;32m    154\u001b[0m                       for i in range(self.pop_size-1)]\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mchilds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigbhours_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbours_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_select_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchilds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melite\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchilds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/essaysense/nlp_adversarial_examples/attacks.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m                                      pop[parent2_idx[i]])\n\u001b[1;32m    154\u001b[0m                       for i in range(self.pop_size-1)]\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mchilds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigbhours_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbours_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_select_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchilds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melite\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchilds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/essaysense/nlp_adversarial_examples/attacks.py\u001b[0m in \u001b[0;36mperturb\u001b[0;34m(self, x_cur, x_orig, neigbhours, neighbours_dist, w_select_probs, target)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mreplace_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_n\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreplace_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_best_replacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigbhours_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbours_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_select_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/essaysense/nlp_adversarial_examples/attacks.py\u001b[0m in \u001b[0;36mselect_best_replacement\u001b[0;34m(self, pos, x_cur, x_orig, target, replace_list)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mreplace_words_and_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_dict\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'UNK'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplace_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0morig_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#             print(replace_words_and_orig)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mreplace_words_lm_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_words_and_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;31m#             print(replace_words_lm_scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#             for i in range(len(replace_words_and_orig)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/essaysense/nlp_adversarial_examples/goog_lm.py\u001b[0m in \u001b[0;36mget_words_probs\u001b[0;34m(self, prefix_words, list_words, suffix)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs_in'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets_in'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_weights_in'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         })\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# print(list_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/p3_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/p3_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/p3_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/p3_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/p3_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/mlcysec/student_directories/securitymonks/p3_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ess_set = 2\n",
    "all_subst = []\n",
    "\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.restore(sess, './models/LSTM_{}_wo_corr'.format(ess_set))\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    tokens = document_level_tokenize(X[X['essay_set']==ess_set]['essay'].iloc[3+10*i])\n",
    "    codes = np.array([dataset.dict.get(token, MAX_VOCAB_SIZE) for token in tokens])\n",
    "    \n",
    "    x_adv = ga_attack.attack(codes, 1)\n",
    "    \n",
    "    subst = dict([(codes[i], x_adv[i]) for i in np.where(x_adv != codes)[0]])\n",
    "    all_subst.append(subst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = np.array([dataset.dict.get(token, MAX_VOCAB_SIZE) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17122"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,   4,  20,  25,  42,  49,  61,  89, 133, 165, 177, 191, 219,\n",
       "        224, 247, 255, 257, 262, 263, 305, 355, 359, 368, 376, 387, 392,\n",
       "        429, 433, 466, 480, 482]),)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x_adv != codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.259047619047619"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array([subst[x] if x in subst.keys() else x for x in codes]) != codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56327522, 0.43672478]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes.predict(sess, [codes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36368513, 0.63631487]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes.predict(sess, [[subst[x] if x in subst.keys() else x for x in codes]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47114098, 0.52885902]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes.predict(sess, [[subst[x] if x in subst.keys() else x for x in codes]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand((0.5288), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand((0.464), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "subst = dict([(codes[i], x_adv[i]) for i in np.where(x_adv != codes)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('then', 'upon'),\n",
       " ('caps', 'ceiling'),\n",
       " ('idea', 'thoughts'),\n",
       " ('offensive', 'abusive'),\n",
       " ('movies', 'films'),\n",
       " ('horrible', 'horrific'),\n",
       " ('might', 'perhaps'),\n",
       " ('about', 'toward'),\n",
       " ('would', 'ought'),\n",
       " ('really', 'indeed'),\n",
       " ('kids', 'youngsters'),\n",
       " ('the', 'to'),\n",
       " ('are', 'constitute'),\n",
       " ('in', 'among'),\n",
       " ('horrible', 'horrific'),\n",
       " ('horrible', 'horrific'),\n",
       " ('to', 'of'),\n",
       " ('need', 'must'),\n",
       " ('to', 'of'),\n",
       " ('music', 'musical'),\n",
       " ('bad', 'wicked'),\n",
       " ('song', 'poem'),\n",
       " ('feeling', 'sentiment'),\n",
       " ('song', 'poem'),\n",
       " ('just', 'merely'),\n",
       " ('different', 'differing'),\n",
       " ('someone', 'person'),\n",
       " ('to', 'of'),\n",
       " ('to', 'of'),\n",
       " ('things', 'elements'),\n",
       " ('would', 'ought')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(full_inv_dict[codes[i]], full_inv_dict[x_adv[i]]) for i in np.where(x_adv != codes)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48877037, 0.51122963]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes.predict(sess, [np.random.permutation(codes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0  --  0.4644466042518616\n",
      "\t\t 1  --  0.4659782648086548\n",
      "\t\t 2  --  0.4659782648086548\n",
      "\t\t 3  --  0.4659782648086548\n",
      "\t\t 4  --  0.46635016798973083\n",
      "\t\t 5  --  0.4711556136608124\n",
      "\t\t 6  --  0.47328993678092957\n",
      "\t\t 7  --  0.47328993678092957\n",
      "\t\t 8  --  0.4748569428920746\n",
      "\t\t 9  --  0.47723349928855896\n",
      "\t\t 10  --  0.47723349928855896\n",
      "\t\t 11  --  0.48222362995147705\n",
      "\t\t 12  --  0.48222362995147705\n",
      "\t\t 13  --  0.4840324819087982\n",
      "\t\t 14  --  0.4843570590019226\n",
      "\t\t 15  --  0.4862484633922577\n",
      "\t\t 16  --  0.4862484633922577\n",
      "\t\t 17  --  0.4868958592414856\n",
      "\t\t 18  --  0.48903757333755493\n",
      "\t\t 19  --  0.48903757333755493\n",
      "\t\t 20  --  0.4898068308830261\n",
      "\t\t 21  --  0.4898068308830261\n",
      "\t\t 22  --  0.490055650472641\n",
      "\t\t 23  --  0.49481308460235596\n",
      "\t\t 24  --  0.49481308460235596\n",
      "\t\t 25  --  0.49526557326316833\n",
      "\t\t 26  --  0.49526557326316833\n",
      "\t\t 27  --  0.4971967935562134\n",
      "\t\t 28  --  0.4984348714351654\n",
      "\t\t 29  --  0.5005375742912292\n"
     ]
    }
   ],
   "source": [
    "x_adv = ga_attack.attack(codes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/LSTM_2_wo_corr\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.restore(sess, './models/LSTM_2_wo_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56327522, 0.43672478]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes.predict(sess, [codes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:06<00:00, 296.96it/s]\n"
     ]
    }
   ],
   "source": [
    "all_codes = []\n",
    "\n",
    "for essay in tqdm(X[X['essay_set']==2]['essay']):\n",
    "    tokens = document_level_tokenize(essay)\n",
    "    codes = np.array([dataset.dict.get(token, MAX_VOCAB_SIZE) for token in tokens])\n",
    "    all_codes.append(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = aes.predict(sess, all_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_range_min = np.array([2, 1, 0, 0, 0, 0, 0, 0])\n",
    "score_range_max = np.array([12, 6, 3, 3, 4, 4, 30, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(scores, essay_set):\n",
    "    mini = np.choose(essay_set-1,score_range_min)\n",
    "    maxi = np.choose(essay_set-1,score_range_max)\n",
    "    \n",
    "    return (scores - mini) / (maxi - mini)\n",
    "\n",
    "def expand(scores, essay_set):\n",
    "    mini = np.choose(essay_set-1,score_range_min)\n",
    "    maxi = np.choose(essay_set-1,score_range_max)\n",
    "    \n",
    "    return np.array(np.round(scores * (maxi - mini) + mini),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7050814553802117"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(expand(predicted[:,1], 2), y[X['essay_set']==2], weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
